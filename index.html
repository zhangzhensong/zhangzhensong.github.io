<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title></title>
<!-- MathJax -->
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
	  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- End MathJax -->
</head>
<body>
<div id="layout-content">
<p><br /></p>
<p><img src="ZhangZhensong.png" width=180 align="right"  style="position:relative;left:0px;top:-10px;padding-right:100px;"/> </p>
<h1><b>Zhensong Zhang</b> （张镇嵩）</h1>
<p>PhD (CUHK), MS (UCAS), BEng (Xidian)<br /></p>
<p>Senior researcher<br />
Huawei Noah's Ark Lab<br />
<b>Email:</b> zhensongzhang[at]hotmail[dot]com
<br /></p>
<h2>Biography </h2>
<p>I joined Huawei Noah's Ark Lab after I obtained my Ph.D. degree from <a href="http://www.cuhk.edu.hk/">The Chinese University of Hong Kong</a> in 2018. Before that, I received a BEng. degree and a M.S. degree from <a href="http://en.xidian.edu.cn/">Xidian University</a> and <a href="http://english.ucas.ac.cn/">University of Chinese Academy of Sciences</a> in 2011 and 2014, respectively. I am currently working on Visual Language Model/AIGC-based image editing. </p>
<p><b>We are looking for self-motivated interns and full times</b>, if you are insterested in doing cool MLLM/AIGC projects, welcome to join us, please drop me an email.</p>
<h2>News</h2>
<div class="infoblock">
<div class="blockcontent">
<ul>
<li><p>[07/2025] One paper is accepted to ICCV 2025, congrats to all coauthors!</p>
</li>
<li><p>[06/2025] We won the second place award in <a href="https://egovis.github.io/cvpr25/#challenges/">HD-EPIC VQA Challenges 2025</a>, congrats to all coauthors!</p>
</li>
<li><p>[04/2025] One paper is accepted to IJCAI 2025, congrats to all coauthors!</p>
</li>
<li><p>[03/2025] One paper is accepted to CVPR 2025, congrats to all coauthors!</p>
</li>
<li><p>[01/2025] One paper is accepted to ICASSP 2025, congrats to all coauthors!</p>
</li>
<li><p>[09/2024] One paper is accepted to NeurIPS 2024, congrats to all coauthors!</p>
</li>
<li><p>[02/2024] Three papers are accepted to CVPR 2024, congrats to all coauthors!</p>
</li>
<li><p>[12/2023] One paper is accepted to ICASSP 2024.</p>
</li>
<li><p>[10/2023] We won the <a href="./imgs/genea2023_award.jpg">Reproducibility Award</a> in <a href="https://genea-workshop.github.io/2023/challenge/">GENEA Challenge 2023</a>.</p>
</li>
<li><p>[09/2023] Our work on robust monocular depth estimation is accepted to IJCV.</p>
</li>
<li><p>[07/2023] Our UnifiedGesture is accepted to ACM MM 2023.</p>
</li>
<li><p>[05/2023] Our DiffuseStyleGesture is accepted to IJCAI 2023.</p>
</li>
<li><p>[05/2023] Our QPGesture is accepted to CVPR 2023 as a highlight.</p>
</li>
<li><p>[11/2022] Our <a href="https://mos-vod-drcn.dbankcdn.cn/P_VT/video_injection/E1ADD9367ACCE5A6FBC57C925A3E1588E57B0D8E834DE49A6160E705F684E6DF1348CB5/v3/M1572381442145518336/MP4Mix_H.264_1920x1080_6000_HEAAC1_PVC_NoCut.mp4?accountinfo=0N4yN1mj5VdH9HFql04pf1UW9GXsQ%2Bw3IF6mQdp8pUqt8GGIGk0BuSVttns0Ka2SXFwtFzQcniSTMCtJCDmqLg%3D%3D%3A20221109033001%3AUTC%2C%2C%2C20221109033001%2C%2C%2C-1%2C1%2C0%2C%2C%2C1%2C%2C%2C%2C1%2C%2C0%2C%2C%2C%2C%2C1%2CEND&amp;GuardEncType=2&amp;contentCode=M202211091161572381437364011778&amp;spVolumeId=MP202211091161572381437020078848&amp;server=mosplay-drcn.himovie.dbankcloud.cn&amp;protocolType=1&amp;formatPriority=504*%2C204*%2C2">sign language avatar</a> appears on <a href="https://developer.huawei.com/consumer/cn/hdc/hdc2022/review/index.html">HDC 2022</a> / <a href="https://www.huawei.com/cn/events/huaweiconnect/shenzhen#livestream">HC 2022</a> and helps translate Chinese keynotes into CSL, <a href="https://mp.weixin.qq.com/s/392jho7iqBca27gNQKTqqA">量子位</a>, <a href="https://xinsheng.huawei.com/next/#/detail?tid=7165161">华为人</a>, <a href="https://mp.weixin.qq.com/s/bIU7r3W88Z03Pg-n0wTTsA">华为开发者联盟服务</a>.</p>
</li>
<li><p>[10/2022] Our joint team  Megatron_RVC won <a href="http://www.robustvision.net/leaderboard.php?benchmark=depth">the RVC 2022 single image depth prediction challenge</a>, <a href="https://dianzi.nwpu.edu.cn/info/1454/14285.htm">news</a></p>
</li>
<li><p>[8/2022] Our human pose and shape estimation paper CLIFF is accepted to ECCV as oral presentation, <a href="https://mp.weixin.qq.com/s/h7sL8ClkknBfGEnfOg8rmg">news</a></p>
</li>
</ul>
</div></div>
<h2>Selected Publications （<a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Zhensong">dblp</a>, <a href="https://scholar.google.com/citations?user=fs8HQxQAAAAJ&amp;hl=zh-CN">Google Scholar</a>）</h2>
<ul>
<li><p>[<b>Preprint</b>] <a href="https://arxiv.org/pdf/2506.18792">ViDAR: Video Diffusion-Aware 4D Reconstruction From Monocular Inputs</a> <br />
Michal Nazarczuk, Sibi Catley-Chandar, Thomas Tanay, <b>Zhensong Zhang</b>, Gregory Slabaugh, Eduardo Pérez-Pellitero<br /></p>
</li>
</ul>
<ul>
<li><p>[<b>Preprint</b>] <a href="https://arxiv.org/pdf/2503.09342?">GASPACHO: Gaussian Splatting for Controllable Humans and Objects</a> <br />
Aymen Mir, Arthur Moreau, Helisa Dhamo, <b>Zhensong Zhang</b>, Eduardo Pérez-Pellitero<br /></p>
</li>
</ul>
<ul>
<li><p>[<b>Preprint</b>] <a href="https://arxiv.org/pdf/2503.09293?">Better Together: Unified Motion Capture and 3D Avatar Reconstruction</a> <br />
Arthur Moreau, Mohammed Brahimi, Richard Shaw, Athanasios Papaioannou, Thomas Tanay, <b>Zhensong Zhang</b>, Eduardo Pérez-Pellitero<br /></p>
</li>
</ul>
<ul>
<li><p>[<b>Preprint</b>] <a href="https://www.techrxiv.org/users/836049/articles/1228135-human-motion-video-generation-a-survey">Human Motion Video Generation: A Survey</a> <br />
Haiwei Xue, Xiangyang Luo, Zhanghao Hu, Xin Zhang, Xunzhi Xiang, Yuqin Dai, Jianzhuang Liu, <b>Zhensong Zhang</b>, Minglei Li, Jian Yang, Fei Ma, Zhiyong Wu, Changpeng Yang, Zonghong Dai, Fei Richard Yu<br />
[<a href="https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation">Website</a>]</p>
</li>
</ul>
<ul>
<li><p>[<b>ICCV</b>] <a href="https://cvpr.thecvf.com/virtual/2025/poster/32990">Frequency-Guided Diffusion for Training-Free Text-Driven Image Translation</a> <br />
Zheng Gao, Jifei Song, <b>Zhensong Zhang</b>, Jiankang Deng, Ioannis Patras <br />
In <i>ICCV</i> 2025.<br /></p>
</li>
</ul>
<ul>
<li><p>[<b>CVPR</b>] <a href="https://cvpr.thecvf.com/virtual/2025/poster/32990">CaricatureBooth: Data-Free Interactive Caricature Generation in a Photo Booth</a> <br />
Zhiyu Qu, Yunqi Miao, <b>Zhensong Zhang</b>, Jifei Song, Jiankang Deng, Yi-Zhe Song <br />
In <i>CVPR</i> 2025.<br /></p>
</li>
</ul>
<ul>
<li><p>[<b>IJCAI</b>] <a href="https://cvpr.thecvf.com/virtual/2025/poster/32990">VideoHumanMIB: Unlocking Appearance Decoupling for Video Human Motion In-betweening</a> <br />
Haiwei Xue, <b>Zhensong Zhang</b>, Minglei Li, Zonghong Dai, Fei Yu, Fei Ma, Zhiyong Wu <br />
In <i>IJCAI</i> 2025.<br /></p>
</li>
</ul>
<ul>
<li><p>[<b>ICASSP</b>] <a href="https://cvpr.thecvf.com/virtual/2025/poster/32990">Identity-Preserving Audio-Driven Holistic Human Motion Video Generation</a> <br />
Haiwei Xue, <b>Zhensong Zhang</b>, Minglei Li, Zonghong Dai<br />
In <i>ICASSP</i> 2025.<br /></p>
</li>
</ul>
<ul>
<li><p>[<b>NeurIPS</b>] <a href="https://arxiv.org/abs/2312.15567">SCRREAM : SCan, Register, REnder And Map: A Framework for Annotating Accurate and Dense 3D Indoor Scenes with a Benchmark</a> <br />
HyunJun Jung, Weihang Li, Shun-Cheng Wu, William Bittner, Nikolas Brasch, Jifei Song, Eduardo Pérez-Pellitero, <b>Zhensong Zhang</b>, Arthur Moreau, Nassir Navab, Benjamin Busam <br />
In <i>NeurIPS</i> 2024.<br />
[<a href="https://github.com/Junggy/SCRREAM">Dataset</a>]</p>
</li>
</ul>
<ul>
<li><p>[<b>CVPR</b>] <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/He_Co-Speech_Gesture_Video_Generation_via_Motion-Decoupled_Diffusion_Model_CVPR_2024_paper.pdf">Co-Speech Gesture Video Generation via Motion-Decoupled Diffusion Model</a> <br />
Xu He, Qiaochu Huang, <b>Zhensong Zhang</b>, Zhiwei Lin, Zhiyong Wu, Sicheng Yang, Minglei Li, Zhiyi Chen, Songcen Xu, Xiaofei Wu <br />
In <i>CVPR</i> 2024.<br />
[<a href="https://github.com/thuhcsi/S2G-MDDiffusion">Code</a>]</p>
</li>
</ul>
<ul>
<li><p>[<b>CVPR</b>] <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Semantics-aware_Motion_Retargeting_with_Vision-Language_Models_CVPR_2024_paper.pdf">Semantics-aware Motion Retargeting with Vision-Language Models</a> <br />
Haodong Zhang, Zhike Chen, Haocheng Xu, Lei Hao, Xiaofei Wu, Songcen Xu, <b>Zhensong Zhang</b>, Yue Wang, Rong Xiong <br />
In <i>CVPR</i> 2024.<br />
[<a href="https://github.com/chenzhike110/Semantic-Motion-Retargeting">Code</a>]</p>
</li>
</ul>
<ul>
<li><p>[<b>CVPR</b>] <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_Low-Res_Leads_the_Way_Improving_Generalization_for_Super-Resolution_by_Self-Supervised_CVPR_2024_paper.pdf">Low-Res Leads the Way: Improving Generalization for Super-Resolution by Self-Supervised Learning</a> <br />
Haoyu Chen, Wenbo Li, Jinjin Gu, Jingjing Ren, Haoze Sun, Xueyi Zou, Youliang Yan, <b>Zhensong Zhang</b>, Lei Zhu <br />
In <i>CVPR</i> 2024.<br />
[<a href="https://haoyuchen.com/LWay">Website</a>]</p>
</li>
</ul>
<ul>
<li><p>[<b>ICASSP</b>] <a href="https://arxiv.org/abs/2312.15567">Conversational Co-Speech Gesture Generation via Modeling Dialog Intention, Emotion, and Context with Diffusion Models</a> <br />
Haiwei Xue, Sicheng Yang, <b>Zhensong Zhang</b>, Zhiyong Wu, Minglei Li, Zonghong Dai, Helen Meng <br />
In <i>ICASSP</i> 2024.<br /></p>
</li>
</ul>
<ul>
<li><p>[<b>IJCV</b>] <a href="https://arxiv.org/abs/2305.04919">Towards A Unified Network for Robust Monocular Depth Estimation: Network Architecture, Training Strategy and Dataset</a> <br />
Mochu Xiang, Yuchao Dai, Feiyu Zhang, Jiawei Shi, Xinyu Tian, <b>Zhensong Zhang</b> <br />
In <i>IJCV</i>.<br /></p>
</li>
</ul>
<ul>
<li><p>[<b>ICMI</b>] <a href="https://arxiv.org/abs/2308.13879">The DiffuseStyleGesture+ Entry to the GENEA Challenge 2023</a> <br />
Sicheng Yang, Haiwei Xue, <b>Zhensong Zhang</b>, Minglei Li, Zhiyong Wu, Xiaofei Wu, Songcen Xu, Zonghong Dai <br />
In <i>ICMI</i> 2023.<br />
[<a href="https://github.com/YoungSeng/DiffuseStyleGesture/tree/DiffuseStyleGesturePlus/BEAT-TWH-main">Code</a>]
[<a href="./imgs/genea2023_award.jpg">Reproducibility Award</a>]</p>
</li>
</ul>
<ul>
<li><p>[<b>ACM MM</b>] <a href="https://arxiv.org/abs/2309.07051">UnifiedGesture: A Unified Gesture Synthesis Model for Multiple Skeletons</a> <br />
Sicheng Yang, Zilin Wang, Zhiyong Wu, Minglei Li, <b>Zhensong Zhang</b>, Qiaochu Huang, Lei Hao, Songcen Xu, Xiaofei Wu, Changpeng Yang, Zonghong Dai <br />
In <i>ACM MM</i> 2023.<br />
[<a href="https://github.com/YoungSeng/UnifiedGesture">Code</a>]
[<a href="https://www.youtube.com/watch?v=Ix22-ZRqSss">Demo</a>]
[<a href="https://www.youtube.com/watch?v=zoWv_SJQ14o">presentation</a>]</p>
</li>
</ul>
<ul>
<li><p>[<b>IJCAI</b>] <a href="https://arxiv.org/abs/2305.04919">DiffuseStyleGesture: Stylized Audio-Driven Co-Speech Gesture Generation with Diffusion Models</a> <br />
Sicheng Yang, Zhiyong Wu, Minglei Li, <b>Zhensong Zhang</b>, Lei Hao, Weihong Bao, Ming Cheng, Long Xiao <br />
In <i>IJCAI</i> 2023.<br />
[<a href="https://github.com/YoungSeng/DiffuseStyleGesture">Code</a>]
[<a href="https://youtu.be/Nzom6gkQ2tM">Demo</a>]</p>
</li>
</ul>
<ul>
<li><p>[<b>CVPR Highlight</b>] <a href="https://arxiv.org/abs/2305.11094">QPGesture: Quantization-Based and Phase-Guided Motion Matching for Natural Speech-Driven Gesture Generation</a> <br />
Sicheng Yang, Zhiyong Wu, Minglei Li, <b>Zhensong Zhang</b>, Lei Hao, Weihong Bao, Haolin Zhuang <br />
In <i>CVPR</i> 2023.<br />
[<a href="https://github.com/YoungSeng/QPGesture">Code</a>]
[<a href="https://www.youtube.com/watch?v=AVqHhnVmGlU">Demo</a>]
[<a href="https://www.youtube.com/watch?v=5GKjFclT618">presentation</a>]</p>
</li>
</ul>
<ul>
<li><p>[<b>ECCV Oral</b>] <a href="https://arxiv.org/abs/2208.00571">CLIFF: Carrying Location Information in Full Frames into Human Pose and Shape Estimation</a> <br />
Zhihao Li, Jianzhuang Liu, <b>Zhensong Zhang</b>, Songcen Xu, Youliang Yan <br />
In <i>ECCV</i> 2022.<br />
[<a href="https://github.com/huawei-noah/noah-research/tree/master/CLiFF">Code</a>]</p>
</li>
</ul>
<ul>
<li><p>[<b>TIP</b>] <a href="https://doi.org/10.1109/TIP.2019.2938086">Multi-View Video Synopsis via Simultaneous Object-Shifting and View-Switching Optimization</a> <br />
<b>Zhensong Zhang</b>, Yongwei Nie, Hanqiu Sun, Qing Zhang, Qiuxia Lai, Guiqing Li, Mingyu Xiao<br />
In <i>IEEE Transactions Image Processing</i>, 2020.</p>
</li>
</ul>
<ul>
<li><p>[<b>TIP</b>] <a href="Collision-Free">Video Synopsis Incorporating Object Speed and Size Changes</a> <br />
Yongwei Nie, Zhenkai Li, <b>Zhensong Zhang</b>, Qing Zhang, Tiezheng Ma, Hanqiu Sun <br />
In <i>IEEE Transactions Image Processing</i>, 2020.</p>
</li>
</ul>
<ul>
<li><p>[<b>TVCG</b>] <a href="https://doi.org/10.1109/TVCG.2019.2923196">Effective Video Stabilization via Joint Trajectory Smoothing and Frame Warping</a><br />
Tiezheng Ma, Yongwei Nie, Qing Zhang, <b>Zhensong Zhang</b>, Hanqiu Sun, Guiqing Li<br />
In <i>IEEE Transactions on Visualization and Computer Graphics</i>, 2020. </p>
</li>
</ul>
<ul>
<li><p>[<b>TCSVT</b>] <a href="https://doi.org/10.1109/TCSVT.2019.2898691">Interactive Contour Extraction via Sketch-Alike Dense-Validation Optimization</a> <br />
Yongwei Nie, Xu Cao, Ping Li, Qing Zhang, <b>Zhensong Zhang</b>, Guiqing Li, Hanqiu Sun<br />
In <i>IEEE Transactions on Circuits Systems for Video Technology</i>, 2020</p>
</li>
</ul>
<ul>
<li><p>[<b>PR</b>] <a href="https://doi.org/10.1016/j.patcog.2019.107139">Video super-resolution via pre-frame constrained and deep-feature enhanced sparse reconstruction</a> <br />
Qiuxia Lai, Yongwei Nie, Hanqiu Sun, Qiang Xu, <b>Zhensong Zhang</b>, Mingyu Xiao <br />
In <i>Pattern Recognition</i>, 2020</p>
</li>
</ul>
<ul>
<li><p>[<b>TIP</b>] <a href="http://ieeexplore.ieee.org/document/8003352/">Dynamic Video Stitching via Shakiness Removing</a> <br />
Yongwei Nie, Tan Su, <b>Zhensong Zhang</b>, Hanqiu Sun and Guiqing Li <br />
In <i>IEEE Transactions on Image Processing</i>, 2018. <br />
[<a href="https://github.com/SuTanTank/VideoStitchingViaShakinessRemoving">Code</a>] [<a href="https://www.youtube.com/watch?v=tx_dbx6VNKc">Demo</a>]</p>
</li>
</ul>
<ul>
<li><p>[<b>SIGGRAPH Asia</b>] <a href="https://dl.acm.org/doi/10.1145/3145749.3149439">Multi-video Object Synopsis Integrating Optimal View Switching</a> <br />
<b>Zhensong Zhang</b>, Yongwei Nie, Hanqiu Sun, Qiuxia Lai and Guiqing Li <br />
In <i>SIGGRAPH Asia 2017 Technical Briefs</i>, Bangkok, Thailand, 2017.</p>
</li>
</ul>
<ul>
<li><p>[<b>TVCG</b>] <a href="http://ieeexplore.ieee.org/document/7593383/">Homography Propagation and Optimization for
Wide-Baseline Street Image Interpolation</a> <br />
Yongwei Nie, <b>Zhensong Zhang</b>, Hanqiu Sun, Tan Su and Guiqing Li <br />
In <i>IEEE Transactions on Visualization and Computer Graphics</i>, 2017. <br />
[<a href="http://nieyongwei.net/download/HomographyPropagation.zip">Code</a>] [<a href="https://youtu.be/gZuqJWvDGOs">Demo</a>]</p>
</li>
</ul>
<ul>
<li><p>[<b>SIGGRAPH Asia</b>] <a href="http://dl.acm.org/citation.cfm?id=3005383">Video Stitching for Handheld Inputs via Combined Video Stabilization</a> <br />
Tan Su, Yongwei Nie, <b>Zhensong Zhang</b>, Hanqiu Sun and Guiqing Li<br />
In <i>SIGGRAPH Asia 2016 Technical Briefs</i>, Macau, China, 2016.</p>
</li>
</ul>
<h2>Honors &amp; Awards</h2>
<ul>
<li><p>Winner, ECCV 2022 RVC monoculer depth estimation prediction challenge <br />


</p>
</li>
</ul>
<h2>Miscellanies</h2>
<ul>
<li><p><b>Paper Lists</b> <br />
<a href="http://kesen.realtimerendering.com/">Computer Graphics Papers</a>,  <a href="https://www.ecva.net/papers.php">ECCV Papers</a>, <a href="https://openaccess.thecvf.com/menu">CVPR/ICCV Papers</a>, <a href="https://papers.nips.cc/">NeurIPS Papers</a></p>
</li>
</ul>
</div>
</body>
</html>
